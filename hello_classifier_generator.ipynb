{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f27a43-cc21-4548-9634-930f3ca5f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b2171b-5112-4dd3-8f9b-a98d4524a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USING MODEL: distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2ForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFGPT2ForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondays are dire\n",
      "[{'label': 'LABEL_0', 'score': 0.17175523936748505}]\n",
      "I like Mondays\n",
      "[{'label': 'LABEL_0', 'score': 0.1624339520931244}]\n",
      "\n",
      "\n",
      "USING MODEL: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondays are dire\n",
      "[{'label': 'NEGATIVE', 'score': 0.9995858073234558}]\n",
      "I like Mondays\n",
      "[{'label': 'POSITIVE', 'score': 0.9704034328460693}]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append(\"distilgpt2\")\n",
    "models.append(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "txts = [\"Mondays are dire\", \"I like Mondays\"]\n",
    "\n",
    "for model in models:\n",
    "    print(\"\\n\\nUSING MODEL: \" + model)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model = model)\n",
    "    for txt in txts:\n",
    "        print(txt)\n",
    "        res = classifier(txt)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f2ed64-3ab8-4329-ad59-213851a383e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USING MODEL: distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Generated Text ------\n",
      "The hills are alive with the sound.\n",
      "\n",
      "In the middle of the desert, there is a stream near the mountains.\n",
      "The sand below is\n",
      "\n",
      "------ End of Generated Text ------\n",
      "\n",
      "\n",
      "------ Generated Text ------\n",
      "The hills are alive with a beautiful scenery in the spring and summer... it will surely be here before the long dark winter sun goes out,\n",
      "\n",
      "------ End of Generated Text ------\n",
      "\n",
      "\n",
      "------ Generated Text ------\n",
      "The hills are alive with all the noise we hear on TV while playing Guitar Hero. But not only have they played the majority of the games we listen\n",
      "\n",
      "------ End of Generated Text ------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "USING MODEL: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "The model 'TFDistilBertForSequenceClassification' is not supported for text-generation. Supported models are ['TFBertLMHeadModel', 'TFCamembertForCausalLM', 'TFCTRLLMHeadModel', 'TFGPT2LMHeadModel', 'TFGPT2LMHeadModel', 'TFGPTJForCausalLM', 'TFMistralForCausalLM', 'TFOpenAIGPTLMHeadModel', 'TFOPTForCausalLM', 'TFRemBertForCausalLM', 'TFRobertaForCausalLM', 'TFRobertaPreLayerNormForCausalLM', 'TFRoFormerForCausalLM', 'TFTransfoXLLMHeadModel', 'TFXGLMForCausalLM', 'TFXLMWithLMHeadModel', 'TFXLMRobertaForCausalLM', 'TFXLNetLMHeadModel'].\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextGenerationPipeline' object has no attribute 'prefix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUSING MODEL: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model)\n\u001b[0;32m----> 3\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe hills are alive with\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1164\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:106\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLNetLMHeadModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransfoXLLMHeadModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     ]:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# For XLNet and TransformerXL we add an article to the prompt to give more state to the model.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextGenerationPipeline' object has no attribute 'prefix'"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"\\n\\nUSING MODEL: \" + model)\n",
    "    generator = pipeline(\"text-generation\", model = model)\n",
    "    results = generator(\"The hills are alive with\", max_length=30, num_return_sequences=3)\n",
    "    for result in results:\n",
    "        print(\"------ Generated Text ------\")\n",
    "        print(result['generated_text'])\n",
    "        print(\"\\n------ End of Generated Text ------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedbfbc-c138-4d86-b3ce-348f21d03039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef8fbb-886d-4576-b417-3d1b2cdd9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
