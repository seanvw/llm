{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2973ebae-2769-493b-a3b6-1b04af905101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# needed to pip install torch (for pytorch)\n",
    "# and of course transformers as docker image for tensorflow was missing this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bc62b4e-1c9f-41bb-bdc5-f99708c52cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USING MODEL: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "Pretty print type(model)\n",
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n",
      "Pretty print type(tokenizer)\n",
      "<class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "\n",
      "Tokens:  {'input_ids': [101, 28401, 2024, 18704, 102, 1045, 2066, 28401, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Pretty print tokens\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101, 28401, 2024, 18704, 102, 1045, 2066, 28401, 102]}\n",
      "\n",
      "Pretty print __dict__\n",
      "{'_encodings': [Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])],\n",
      " '_n_sequences': 2,\n",
      " 'data': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          'input_ids': [101, 28401, 2024, 18704, 102, 1045, 2066, 28401, 102]}}\n",
      "\n",
      "Pretty print type(tokens)\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      " Mondays are dire\n",
      "[{'label': 'NEGATIVE', 'score': 0.9995858073234558}]\n",
      "\n",
      " I like Mondays\n",
      "[{'label': 'POSITIVE', 'score': 0.9704034328460693}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a dive into strcuctures and classes \n",
    "\n",
    "model_name = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "            \n",
    "#model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "print(\"\\n\\nUSING MODEL: \" + model_name)\n",
    "\n",
    "txt1 = \"Mondays are dire\"\n",
    "txt2 = \"I like Mondays\"\n",
    "txts = [txt1, txt2]\n",
    "\n",
    "# explict model and tokenzier extraction from model name\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#model = DistilBertForSequenceClassification.from_pretrained(mode_name)\n",
    "print(\"Pretty print type(model)\")\n",
    "pprint(type(model))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "print(\"Pretty print type(tokenizer)\")\n",
    "pprint(type(tokenizer))\n",
    "\n",
    "\n",
    "# couldn't get mask from tokenize or encode\n",
    "tokens = tokenizer.encode_plus(txts, return_attention_mask=True, verbose=True)\n",
    "print()\n",
    "print(\"Tokens: \", tokens)\n",
    "print()\n",
    "print(\"Pretty print tokens\")\n",
    "pprint(tokens)\n",
    "print()\n",
    "print(\"Pretty print __dict__\")\n",
    "pprint(tokens.__dict__)\n",
    "print()\n",
    "print(\"Pretty print type(tokens)\")\n",
    "pprint(type(tokens))\n",
    "print()\n",
    "print(\"Attention mask:\", tokens.attention_mask)\n",
    "print()\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "for txt in txts:\n",
    "    print(\"\\n\",txt)\n",
    "    res = classifier(txt)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00e080-5b28-48dd-acaa-105f947e9607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ee909d-cf06-44a3-a47d-0757cb60bc0e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60eb5c3-24d9-43c3-a0b6-1c5ec07fc725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
