{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP43f1UBldsj7TrUQ9OEupp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Example of using transformers and torch libraries with English and German models**"
      ],
      "metadata": {
        "id": "JpfQceBWat3D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "8afxa4URDHhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0a2cc4-7fd8-4f9b-8ede-2136c000ccf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With explicit model and tokenizer\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Classification results, using distilbert/distilbert-base-uncased-finetuned-sst-2-english are:\n",
            "\n",
            "Happy days with Colab, we can use GPUs: sentiment is POSITIVE, probability is 0.9993423819541931\n",
            "Our Intel CPU/GPU combo is not compatible with CUDA: sentiment is NEGATIVE, probability is 0.9997064471244812\n",
            "She drives a green car: sentiment is POSITIVE, probability is 0.9920748472213745\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pprint\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# classify some sentences and print results\n",
        "model_name = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if (True):\n",
        "  # explicit\n",
        "  print(\"With explicit model and tokenizer\")\n",
        "  classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "else:\n",
        "  # just pass the model name\n",
        "  print(\"Classifier and tokeizer from model name\")\n",
        "  classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "sentences = [\"Happy days with Colab, we can use GPUs\",\n",
        "             \"Our Intel CPU/GPU combo is not compatible with CUDA\",\n",
        "             \"She drives a green car\"]\n",
        "results = classifier(sentences)\n",
        "n_dash = 120\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Classification results, using {model_name} are:\\n\")\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"{sentences[i]}: sentiment is {r['label']}, probability is {r['score']}\")\n",
        "print(\"-\" * n_dash)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see tokens and ids\n",
        "eg_sentence = sentences[0]\n",
        "tokens = tokenizer.tokenize(eg_sentence)\n",
        "#print(tokens)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "#print(token_ids)\n",
        "\n",
        "words = eg_sentence.split()\n",
        "words_n = len(words)\n",
        "tokens_n = len(tokens)\n",
        "print(\"A look at tokens with example\")\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Example sentence \\\"{eg_sentence}\\\" has {words_n} words, {tokens_n} tokens\\n\")\n",
        "for i, t in enumerate(tokens):\n",
        "  print(f\"{i}: {tokens[i]} {token_ids[i]}\")\n",
        "print(\"-\" * n_dash)\n",
        "\n",
        "print()\n",
        "print(\"Regenerate tokens\")\n",
        "print(\"-\" * n_dash)\n",
        "regen_ids = tokenizer(eg_sentence)\n",
        "input_ids_n = len(regen_ids['input_ids'])\n",
        "print(f\"Regenerated sentence is {input_ids_n} tokens and is associated with an attention mask - it's all you need!\\n\")\n",
        "print('Regenerated IDs',regen_ids['input_ids'])\n",
        "print('Attention mask',regen_ids['attention_mask'])\n",
        "diffs = [x for x in regen_ids['input_ids'] if x not in token_ids]\n",
        "print(f\"\\nDifferences are the start and stop tokens:\")\n",
        "for d in diffs:\n",
        "  print(d)\n",
        "print(\"-\" * n_dash)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV9D12MYasgS",
        "outputId": "e20d83ba-a208-4596-9444-153598b922ad"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A look at tokens with example\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Example sentence \"Happy days with Colab, we can use GPUs\" has 8 words, 11 tokens\n",
            "\n",
            "0: happy 3407\n",
            "1: days 2420\n",
            "2: with 2007\n",
            "3: cola 15270\n",
            "4: ##b 2497\n",
            "5: , 1010\n",
            "6: we 2057\n",
            "7: can 2064\n",
            "8: use 2224\n",
            "9: gp 14246\n",
            "10: ##us 2271\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Regenerate tokens\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Regenerated sentence is 13 tokens and is associated with an attention mask - it's all you need!\n",
            "\n",
            "Regenerated IDs [101, 3407, 2420, 2007, 15270, 2497, 1010, 2057, 2064, 2224, 14246, 2271, 102]\n",
            "Attention mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Differences are the start and stop tokens:\n",
            "101\n",
            "102\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of prepping sentences for training, X_train is common name for list of texts\n",
        "X_train = sentences\n",
        "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGcOPKDgdat",
        "outputId": "562adaf1-bc67-4f49-8ce7-b409fe257983"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  3407,  2420,  2007, 15270,  2497,  1010,  2057,  2064,  2224,\n",
            "         14246,  2271,   102,     0,     0],\n",
            "        [  101,  2256, 13420, 17368,  1013, 14246,  2226, 25025,  2003,  2025,\n",
            "         11892,  2007, 12731,  2850,   102],\n",
            "        [  101,  2016,  9297,  1037,  2665,  2482,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack first level\n",
        "\n",
        "# gives keys\n",
        "print(\"Keys: \\n\", *batch)\n",
        "\n",
        "# keys and values\n",
        "print(\"Keys and values: \\n\")\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAES98-ZpmZG",
        "outputId": "2bbc2693-aeef-45ec-a944-920d51198835"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: \n",
            " input_ids attention_mask\n",
            "Keys and values: \n",
            "\n",
            "input_ids torch.Size([3, 15])\n",
            "attention_mask torch.Size([3, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# going deeper with structures\n",
        "with torch.no_grad():\n",
        "    if (True):\n",
        "      # with labels, we see the loss\n",
        "      outs = model(**batch, labels=torch.tensor([1, 0]))\n",
        "    if (False):\n",
        "      # loss is None\n",
        "      outs = model(**batch)\n",
        "    print(\"Output object:\\n\")\n",
        "    pprint.pprint(outs)\n",
        "    print()\n",
        "\n",
        "    # softmax on logits tensor\n",
        "    preds = F.softmax(outs.logits, dim=1)\n",
        "    print(\"Predictions tensor:\", preds, \"\\n\")\n",
        "\n",
        "    # labels via argmax on predictions\n",
        "    labels = torch.argmax(preds, dim=1)\n",
        "    print(\"Labels tensor:\",labels, \"\\n\")\n",
        "\n",
        "    labels_words = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
        "    print(\"Labels as words:\",labels_words, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3tKCmEy8oR4m",
        "outputId": "76b3679e-304b-454a-b6e4-aca2b9f84354"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (3) to match target batch_size (2).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-91fc39eb97d7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;31m# with labels, we see the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;31m# loss is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3) to match target batch_size (2)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recursive list, no dot file dirs\n",
        "def list_files_and_directories(path):\n",
        "    for item in os.listdir(path):\n",
        "        item_path = os.path.join(path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            print(item_path)\n",
        "        elif os.path.isdir(item_path) and not item.startswith('.'):\n",
        "            print(item_path)\n",
        "            list_files_and_directories(item_path)\n",
        "\n",
        "list_files_and_directories('.')\n",
        ""
      ],
      "metadata": {
        "id": "yA2LzOuFzR4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving\n",
        "save_dir = 'saved_model'\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "model.save_pretrained(save_dir)\n",
        "# list_files_and_directories(save_dir)\n",
        "\n",
        "# get a long listing with size\n",
        "output = subprocess.check_output(['ls', '-Rlh',save_dir])\n",
        "print(output.decode('utf-8'))\n",
        "\n",
        "# re-read\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n"
      ],
      "metadata": {
        "id": "m3cjIVKIxZym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# german\n",
        "model_name_de = 'oliverguhr/german-sentiment-bert'\n",
        "tokenizer_de = AutoTokenizer.from_pretrained(model_name_de)\n",
        "model_de = AutoModelForSequenceClassification.from_pretrained(model_name_de)\n",
        "\n",
        "saetze = [\"Wo ist mein Handy?\", \"Ich bin ein Freiburger\",\n",
        "         \"Was soll der Scheiß?\",\"Ich bin glücklich\",\n",
        "         \"Sie fährt ein grünes Auto\",\n",
        "         \"Sie fährt ein klimafreundliches Auto\"]\n",
        "\n",
        "# explicit\n",
        "print(\"With explicit model and tokenizer\")\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_de, tokenizer=tokenizer_de)\n",
        "results = classifier(saetze)\n",
        "#print(results)\n",
        "n_dash = 120\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Classification results, using {model_name_de} are:\\n\")\n",
        "for i, r in enumerate(results):\n",
        "  print(f\"{saetze[i]}: sentiment is {r['label']}, probability is {r['score']}\")\n",
        "print(\"-\" * n_dash)\n",
        "\n",
        "print(\"Note: Interesting, driving a green car and even a climate friendly car is neutral in German! It's strongly positive in English.\")\n"
      ],
      "metadata": {
        "id": "uPmIEqLf0QPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qx1tlmoXECqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}