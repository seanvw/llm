{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlYmvSoOtZ28nlrm86pOEY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Example of using transformers and torch libraries with English and German models**"
      ],
      "metadata": {
        "id": "JpfQceBWat3D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "8afxa4URDHhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d85ea68-0c59-4ee0-bd4a-a2f48bb532c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With explicit model and tokenizer\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Classification results, using distilbert/distilbert-base-uncased-finetuned-sst-2-english are:\n",
            "\n",
            "Happy days with Colab, we can use GPUs.: sentiment is POSITIVE, probability is 0.9986945986747742\n",
            "Our Intel CPU/GPU combo is not compatible with CUDA: sentiment is NEGATIVE, probability is 0.9997064471244812\n",
            "She drives a green car: sentiment is POSITIVE, probability is 0.9920748472213745\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pprint\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# classify some sentences and print results\n",
        "model_name = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if (True):\n",
        "  # explicit\n",
        "  print(\"With explicit model and tokenizer\")\n",
        "  classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "else:\n",
        "  # just pass the model name\n",
        "  print(\"Classifier and tokeizer from model name\")\n",
        "  classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "sentences = [\"Happy days with Colab, we can use GPUs.\",\n",
        "             \"Our Intel CPU/GPU combo is not compatible with CUDA\",\n",
        "             \"She drives a green car\"]\n",
        "results = classifier(sentences)\n",
        "n_dash = 120\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Classification results, using {model_name} are:\\n\")\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"{sentences[i]}: sentiment is {r['label']}, probability is {r['score']}\")\n",
        "print(\"-\" * n_dash)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see tokens and ids\n",
        "eg_sentence = sentences[0]\n",
        "tokens = tokenizer.tokenize(eg_sentence)\n",
        "#print(tokens)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "#print(token_ids)\n",
        "\n",
        "words = eg_sentence.split()\n",
        "words_n = len(words)\n",
        "tokens_n = len(tokens)\n",
        "print(\"A look at tokens with a single example sentence\")\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Example sentence \\\"{eg_sentence}\\\" has {words_n} words, {tokens_n} tokens\\n\")\n",
        "for i, t in enumerate(tokens):\n",
        "  print(f\"{i}: {tokens[i]} {token_ids[i]}\")\n",
        "print(\"-\" * n_dash)\n",
        "\n",
        "print()\n",
        "print(\"Regenerate tokens\")\n",
        "print(\"-\" * n_dash)\n",
        "regen_ids = tokenizer(eg_sentence)\n",
        "input_ids_n = len(regen_ids['input_ids'])\n",
        "print(f\"Regenerated sentence is {input_ids_n} tokens and is associated with an attention mask - it's all you need!\\n\")\n",
        "print('Regenerated IDs',regen_ids['input_ids'])\n",
        "print('Attention mask',regen_ids['attention_mask'])\n",
        "diffs = [x for x in regen_ids['input_ids'] if x not in token_ids]\n",
        "print(f\"\\nDifferences are the start and stop tokens:\")\n",
        "for d in diffs:\n",
        "  print(d)\n",
        "print(\"-\" * n_dash)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV9D12MYasgS",
        "outputId": "fd547943-a8ee-412a-be10-692bcfcd3693"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A look at tokens with a single example sentence\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Example sentence \"Happy days with Colab, we can use GPUs.\" has 8 words, 12 tokens\n",
            "\n",
            "0: happy 3407\n",
            "1: days 2420\n",
            "2: with 2007\n",
            "3: cola 15270\n",
            "4: ##b 2497\n",
            "5: , 1010\n",
            "6: we 2057\n",
            "7: can 2064\n",
            "8: use 2224\n",
            "9: gp 14246\n",
            "10: ##us 2271\n",
            "11: . 1012\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Regenerate tokens\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Regenerated sentence is 14 tokens and is associated with an attention mask - it's all you need!\n",
            "\n",
            "Regenerated IDs [101, 3407, 2420, 2007, 15270, 2497, 1010, 2057, 2064, 2224, 14246, 2271, 1012, 102]\n",
            "Attention mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Differences are the start and stop tokens:\n",
            "101\n",
            "102\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of prepping sentences for training, X_train is common name for list of texts\n",
        "X_train = sentences\n",
        "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGcOPKDgdat",
        "outputId": "e5261fb7-cc4a-4416-b473-571daa6dd451"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  3407,  2420,  2007, 15270,  2497,  1010,  2057,  2064,  2224,\n",
            "         14246,  2271,  1012,   102,     0],\n",
            "        [  101,  2256, 13420, 17368,  1013, 14246,  2226, 25025,  2003,  2025,\n",
            "         11892,  2007, 12731,  2850,   102],\n",
            "        [  101,  2016,  9297,  1037,  2665,  2482,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack first level\n",
        "\n",
        "# gives keys\n",
        "print(\"Keys: \\n\", *batch)\n",
        "\n",
        "# keys and values\n",
        "print(\"Keys and values: \\n\")\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAES98-ZpmZG",
        "outputId": "2b377ef7-12c6-4c74-d0f4-11a55955452f"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: \n",
            " input_ids attention_mask\n",
            "Keys and values: \n",
            "\n",
            "input_ids torch.Size([3, 15])\n",
            "attention_mask torch.Size([3, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(batch)\n",
        "\n",
        "# going deeper with structures\n",
        "with torch.no_grad():\n",
        "  if (False):\n",
        "    # with labels, we see the loss\n",
        "    # dependance on tensor size?!\n",
        "    outs = model(**batch, labels=torch.tensor([1, 0]))\n",
        "  else:\n",
        "    # loss is None\n",
        "    outs = model(**batch)\n",
        "\n",
        "  print(\"Output object:\\n\")\n",
        "  pprint.pprint(outs)\n",
        "  print()\n",
        "\n",
        "  # softmax on logits tensor\n",
        "  preds = F.softmax(outs.logits, dim=1)\n",
        "  print(\"Predictions tensor:\", preds, \"\\n\")\n",
        "\n",
        "  # labels via argmax on predictions\n",
        "  labels = torch.argmax(preds, dim=1)\n",
        "  print(\"Labels tensor:\",labels, \"\\n\")\n",
        "\n",
        "  labels_words = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
        "  print(\"Labels as words:\",labels_words, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tKCmEy8oR4m",
        "outputId": "a7e5587f-4d52-416e-f99c-b87d7efbc936"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  3407,  2420,  2007, 15270,  2497,  1010,  2057,  2064,  2224,\n",
            "         14246,  2271,  1012,   102,     0],\n",
            "        [  101,  2256, 13420, 17368,  1013, 14246,  2226, 25025,  2003,  2025,\n",
            "         11892,  2007, 12731,  2850,   102],\n",
            "        [  101,  2016,  9297,  1037,  2665,  2482,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "Output object:\n",
            "\n",
            "SequenceClassifierOutput(loss=None,\n",
            "                         logits=tensor([[-3.2223,  3.4176],\n",
            "        [ 4.5285, -3.6047],\n",
            "        [-2.3882,  2.4416]]),\n",
            "                         hidden_states=None,\n",
            "                         attentions=None)\n",
            "\n",
            "Predictions tensor: tensor([[1.3054e-03, 9.9869e-01],\n",
            "        [9.9971e-01, 2.9354e-04],\n",
            "        [7.9251e-03, 9.9207e-01]]) \n",
            "\n",
            "Labels tensor: tensor([1, 0, 1]) \n",
            "\n",
            "Labels as words: ['POSITIVE', 'NEGATIVE', 'POSITIVE'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recursive list, no dot file dirs\n",
        "def list_files_and_directories(path):\n",
        "    for item in os.listdir(path):\n",
        "        item_path = os.path.join(path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            print(item_path)\n",
        "        elif os.path.isdir(item_path) and not item.startswith('.'):\n",
        "            print(item_path)\n",
        "            list_files_and_directories(item_path)\n",
        "\n",
        "list_files_and_directories('.')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA2LzOuFzR4U",
        "outputId": "c9027d3d-e07f-42bc-dbd2-468e65d19893"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./saved_model\n",
            "./saved_model/tokenizer.json\n",
            "./saved_model/model.safetensors\n",
            "./saved_model/special_tokens_map.json\n",
            "./saved_model/vocab.txt\n",
            "./saved_model/config.json\n",
            "./saved_model/tokenizer_config.json\n",
            "./sample_data\n",
            "./sample_data/anscombe.json\n",
            "./sample_data/README.md\n",
            "./sample_data/california_housing_train.csv\n",
            "./sample_data/mnist_train_small.csv\n",
            "./sample_data/california_housing_test.csv\n",
            "./sample_data/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving\n",
        "save_dir = 'saved_model'\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "model.save_pretrained(save_dir)\n",
        "# list_files_and_directories(save_dir)\n",
        "\n",
        "# get a long listing with size\n",
        "output = subprocess.check_output(['ls', '-Rlh',save_dir])\n",
        "print(output.decode('utf-8'))\n",
        "\n",
        "# re-read\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3cjIVKIxZym",
        "outputId": "58991fcd-1310-4d33-c16c-1eab5bff4c5c"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_model:\n",
            "total 257M\n",
            "-rw-r--r-- 1 root root  774 Dec 12 15:32 config.json\n",
            "-rw-r--r-- 1 root root 256M Dec 12 15:32 model.safetensors\n",
            "-rw-r--r-- 1 root root  125 Dec 12 15:32 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.3K Dec 12 15:32 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 695K Dec 12 15:32 tokenizer.json\n",
            "-rw-r--r-- 1 root root 227K Dec 12 15:32 vocab.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# german\n",
        "model_name_de = 'oliverguhr/german-sentiment-bert'\n",
        "tokenizer_de = AutoTokenizer.from_pretrained(model_name_de)\n",
        "model_de = AutoModelForSequenceClassification.from_pretrained(model_name_de)\n",
        "\n",
        "saetze = [\"Wo ist mein Handy?\", \"Ich bin ein Freiburger\",\n",
        "         \"Was soll der Scheiß?\",\"Ich bin glücklich\",\n",
        "         \"Sie fährt ein grünes Auto\",\n",
        "         \"Sie fährt ein klimafreundliches Auto\"]\n",
        "\n",
        "# explicit\n",
        "print(\"With explicit model and tokenizer\")\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_de, tokenizer=tokenizer_de)\n",
        "results = classifier(saetze)\n",
        "#print(results)\n",
        "n_dash = 120\n",
        "print(\"-\" * n_dash)\n",
        "print(f\"Classification results, using {model_name_de} are:\\n\")\n",
        "for i, r in enumerate(results):\n",
        "  print(f\"{saetze[i]}: sentiment is {r['label']}, probability is {r['score']}\")\n",
        "print(\"-\" * n_dash)\n",
        "\n",
        "print(\"Note: Interesting, driving a green car and even a climate friendly car is neutral in German! It's strongly positive in English.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPmIEqLf0QPw",
        "outputId": "d3b464b3-8acf-4104-be5a-d03dbe2817a3"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With explicit model and tokenizer\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Classification results, using oliverguhr/german-sentiment-bert are:\n",
            "\n",
            "Wo ist mein Handy?: sentiment is negative, probability is 0.5165525078773499\n",
            "Ich bin ein Freiburger: sentiment is negative, probability is 0.9228449463844299\n",
            "Was soll der Scheiß?: sentiment is negative, probability is 0.9580993056297302\n",
            "Ich bin glücklich: sentiment is positive, probability is 0.9781695604324341\n",
            "Sie fährt ein grünes Auto: sentiment is neutral, probability is 0.9964893460273743\n",
            "Sie fährt ein klimafreundliches Auto: sentiment is neutral, probability is 0.9915153980255127\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Note: Interesting, driving a green car and even a climate friendly car is neutral in German! It's strongly positive in English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qx1tlmoXECqr"
      },
      "execution_count": 185,
      "outputs": []
    }
  ]
}